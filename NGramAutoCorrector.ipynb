{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2ae2cc-8b1c-4425-9181-3bc31947a4e0",
   "metadata": {},
   "source": [
    "# Basic AutoCorrection and BiGram AutoCorrection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d3cab859-ad39-420f-9006-bde746584fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa4485ae-3489-4e88-a4ef-1637b8b422f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('final.txt', 'r', encoding='utf-8') as f:\n",
    "    file = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0e2025d-b544-4c75-ad02-f6c70de8bb63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(lines):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    words = []        \n",
    "    for line in lines:\n",
    "        line = line.strip().lower()\n",
    "        word = re.findall(r'\\w+', line)\n",
    "        words.extend(word)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4c3b275-79c6-445a-8071-636c9c0e2fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['copyright', 'laws', 'are', 'changing', 'all', 'over', 'the', 'world', 'be', 'sure']\n",
      "There are 66684 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "word_l = process_data(file)\n",
    "vocab = set(word_l)\n",
    "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b4b5d-1344-4bef-863b-4357ece44671",
   "metadata": {},
   "source": [
    "<p>\n",
    "    This auto-correct architecture has 4 components - <br>\n",
    " 1) Filtering Mispells : One simple approach could be checking if a word is there in the vocabulary or not. <br>\n",
    "2) Word Suggestion Mechanism : This mechnism suggests candidate words based on deletion, insertion, switch or replace of one/two characters in the original word. <br>\n",
    "3) Probability Distribution Mechanism : The probability distribution {key(word) : value(probability)} is created calculated using a large text corpus. Probability of each candidate is found using this distribution and the most probable candidate is the final one. <br>\n",
    "4) Replace Mispells : Simple replace the mispelled word with the most probable suggestion. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e5b45b-14f8-4d59-95ab-5c39bd451e87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Artchitecture Part 1 : (Filtering Mispells)\n",
    "<p> A function that tokenizes the sentences and checks the availability of each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3485663d-a671-4898-b791-0d68ccde62d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_wrong_word(sent, vocab):\n",
    "    wrong_words = []\n",
    "    sent = sent.strip().lower().split(\" \")\n",
    "    for word in sent:\n",
    "        if word not in vocab:\n",
    "            wrong_words.append(word)\n",
    "    return wrong_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c972b3a-6c16-4fec-8371-f453cdadb8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['goinng']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_wrong_word('he is goinng home', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530a397-fd6d-4e43-adb4-8eedde731d29",
   "metadata": {},
   "source": [
    "### Architecture Part 2 : (Word Suggestion Mechanism)\n",
    "We'll impliment separate functions of each of the steps (deletion, insertion, switching, replace) and then combine them to edit one or two letter of the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ff52cdc-81da-42bd-a4ba-479ab5fd876c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    delete_l = [s[0]+s[1][1:] for s in split_l]\n",
    "    if verbose: print(f\"input word : {word} \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a9290434-56fd-409d-a401-32b5e77072d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word : cans \n",
      "split_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \n",
      "delete_l = ['ans', 'cns', 'cas', 'can']\n"
     ]
    }
   ],
   "source": [
    "delete_word_l = delete_letter(word=\"cans\", \n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "284a266f-5259-4244-84a7-1ad9b4a9cfe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    for s in split_l:\n",
    "        if len(s[1])>2:\n",
    "            temp = s[0] + s[1][1] + s[1][0] + s[1][2:]\n",
    "        elif len(s[1]) == 2:\n",
    "            temp = s[0] + s[1][1] + s[1][0]\n",
    "        elif len(s[1]) == 1:\n",
    "            continue\n",
    "        switch_l.append(temp)\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "\n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb3b74ba-db12-46d0-82fb-12b5f56f7d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = eta \n",
      "split_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \n",
      "switch_l = ['tea', 'eat']\n"
     ]
    }
   ],
   "source": [
    "switch_word_l = switch_letter(word=\"eta\",\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "025909ce-1281-494a-9203-f1c6e3b56237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    for s in split_l:\n",
    "        if len(s[1]) == 1:\n",
    "            for l in letters:\n",
    "                if l != s[1][0]:\n",
    "                    temp = l\n",
    "                    replace_l.append(s[0]+temp)\n",
    "        elif len(s) > 1:\n",
    "            for l in letters:\n",
    "                if l != s[1][0]:\n",
    "                    temp = l + s[1][1:]\n",
    "                    replace_l.append(s[0]+temp)\n",
    "        \n",
    "    replace_set = set(replace_l)\n",
    "    \n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d08716ec-43c1-49cd-80c1-c376e569575a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = can \n",
      "split_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \n",
      "replace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n"
     ]
    }
   ],
   "source": [
    "replace_l = replace_letter(word='can',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b7c4f10-f6b7-464d-810c-946f6cd35880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs of switch_letter('at') is 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of outputs of switch_letter('at') is {len(switch_letter('fate'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f82c9aea-cb5f-4107-9863-72c6995f2256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    for s in split_l:\n",
    "        for l in letters:\n",
    "            insert_l.append(s[0]+l+s[1])\n",
    "\n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e84f478-ad15-49b0-a734-c4175955d785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word at \n",
      "split_l = [('', 'at'), ('a', 't'), ('at', '')] \n",
      "insert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\n",
      "Number of strings output by insert_letter('at') is 78\n"
     ]
    }
   ],
   "source": [
    "insert_l = insert_letter('at', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272dea9f-5b20-4e71-859e-fdf59a29706e",
   "metadata": {},
   "source": [
    "##### Let's combine these individual steps and impliment two function for for editing on or two characters from a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac435989-241f-456c-91ed-49616e8079b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "    insert_l = insert_letter(word)\n",
    "    delete_l = delete_letter(word)\n",
    "    replace_l = replace_letter(word)\n",
    "    switch_l = switch_letter(word)\n",
    "    \n",
    "    if allow_switches:\n",
    "        ans = insert_l + delete_l + replace_l + switch_l\n",
    "    else:\n",
    "        ans = insert_l + delete_l + replace_l\n",
    "        \n",
    "    edit_one_set = set(ans)\n",
    "\n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0e8e9a79-413c-4151-9661-b3cbcfef8520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word : at \n",
      "edit_one_l \n",
      "['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']\n",
      "\n",
      "Number of outputs from edit_one_letter('at') is 129\n"
     ]
    }
   ],
   "source": [
    "tmp_word = \"at\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word : {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "#print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c4dc9fd-baca-4733-b54d-cd9a0e989beb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "    one_edit = edit_one_letter(word)\n",
    "    ans = []\n",
    "    for w in one_edit:\n",
    "        ans.append(w)\n",
    "        ans.extend(edit_one_letter(w))\n",
    "        \n",
    "    edit_two_set = set(ans)\n",
    "    \n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e400e6b2-aa98-4b12-a7d9-5e6d39bbe262",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings with edit distance of two: 2654\n",
      "First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']\n",
      "Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']\n",
      "The data type of the returned object should be a set <class 'set'>\n",
      "Number of strings that are 2 edit distances from 'at' is 7154\n"
     ]
    }
   ],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e38f79-275b-473a-a2bf-d695bcb5698b",
   "metadata": {},
   "source": [
    "### Architecture Part 3 : (Probability Distribution)\n",
    "We'll calculate the frequecies of each word using the corpus that we have. Then we'll divide each frequencies by word count to find the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0e2787fb-69b0-416d-907d-b0469b6a5228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    word_count_dict = {}  \n",
    "    word_count_dict = Counter(word_l)\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "655df9b3-e965-40ad-86db-0c77f71aa665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 66684 key values pairs\n",
      "The count for the word 'thee' is 28\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count(word_l)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bd6c61e4-c034-4d02-bccf-41b0b806e3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {} \n",
    "    total = 1\n",
    "    for word in word_count_dict.keys():\n",
    "        total = total + word_count_dict[word]\n",
    "        \n",
    "    for word in word_count_dict.keys():\n",
    "        probs[word] = word_count_dict[word]/total\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8f8c4a51-9ae7-45cb-bfa9-1f996838e3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 66684\n",
      "P('you') is 0.0048\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('you') is {probs['you']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e8b0e694-56d1-4846-9451-a2642b9144a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48985</th>\n",
       "      <td>interotes</td>\n",
       "      <td>8.584988e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37440</th>\n",
       "      <td>campains</td>\n",
       "      <td>1.716998e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35009</th>\n",
       "      <td>apparient</td>\n",
       "      <td>8.584988e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49720</th>\n",
       "      <td>lauge</td>\n",
       "      <td>8.584988e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36793</th>\n",
       "      <td>berrys</td>\n",
       "      <td>8.584988e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word   probability\n",
       "48985  interotes  8.584988e-07\n",
       "37440   campains  1.716998e-06\n",
       "35009  apparient  8.584988e-07\n",
       "49720      lauge  8.584988e-07\n",
       "36793     berrys  8.584988e-07"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df = pd.DataFrame({'word':probs.keys(), 'probability':probs.values()}).sort_values(by='probability', ascending=False)\n",
    "prob_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "063fd8f8-18bb-46ff-b7cc-410769ff72e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='word'>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob_df.head().plot.bar(x='word', y='probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd167532-97c7-41a7-82bf-e46eb9ba7535",
   "metadata": {},
   "source": [
    "### Architecture Part 4 : (Replace Misspells with correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bcedcf07-7207-4af1-908f-565c713d0036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "   \n",
    "    if word in probs.keys():\n",
    "        suggestions.append(word)\n",
    "    for w in edit_one_letter(word):\n",
    "        if len(suggestions) == n:\n",
    "            break\n",
    "        if w in probs.keys():\n",
    "            suggestions.append(w)\n",
    "    for w in edit_two_letters(word):\n",
    "        if len(suggestions) == n:\n",
    "            break\n",
    "        if w in probs.keys():\n",
    "            suggestions.append(w)\n",
    "        \n",
    "    best_words = {}\n",
    "    \n",
    "    for s in suggestions:\n",
    "        best_words[s] = probs[s]\n",
    "        \n",
    "    best_words = sorted(best_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    n_best = best_words \n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0f55ffcb-f81a-48a1-8296-2638c0177bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_correct_word(word, vocab, probs, n): \n",
    "    corrections = get_corrections(word, probs, vocab, n, verbose=False)\n",
    "#    print(corrections)\n",
    "    if len(corrections) == 0:\n",
    "        return word\n",
    "    \n",
    "    final_word = corrections[0][0]\n",
    "    final_prob = corrections[0][1]\n",
    "    for i, word_prob in enumerate(corrections):\n",
    "        #print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "        if word_prob[1] > final_prob:\n",
    "            final_word = word_prob[0]\n",
    "            final_prob = word_prob[1]\n",
    "    return final_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3f48189-472c-4f3c-ae4f-22a3dd8024f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_correct_word('annd', vocab, probs, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83768f-f2cc-46a4-9d4f-3a930e692488",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "97ebcc56-2b78-4d8c-809c-1dee3711cb47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autocorrect(sentence, vocab, probs):\n",
    "    print(\"Input sentence : \", sentence)\n",
    "    wrong_words = find_wrong_word(sentence, vocab)\n",
    "    print(\"Wrong words : \", wrong_words)\n",
    "    #print(wrong_words)\n",
    "    correct_words = []\n",
    "    for word in sentence.strip().lower().split(\" \"):\n",
    "        if word in wrong_words:\n",
    "            correct_word = get_correct_word(word, vocab, probs, 15)\n",
    "            #print(word, correct_word)\n",
    "            word = correct_word\n",
    "        correct_words.append(word)\n",
    "    print(\"Output Sentence : \", \" \".join(correct_words).capitalize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c0967-9d9a-4f34-b91b-baea80e13a53",
   "metadata": {},
   "source": [
    "### DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dd169621-125a-4aa8-b2b5-8bdb35a52102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  he is goinng home\n",
      "Wrong words :  ['goinng']\n",
      "Output Sentence :  He is going home\n"
     ]
    }
   ],
   "source": [
    "autocorrect(\"he is goinng home\", vocab, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ba1752d8-4075-4e7f-af27-0a5ef1e398d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  I seeı it\n",
      "Wrong words :  ['seeı']\n",
      "Output Sentence :  I see it\n"
     ]
    }
   ],
   "source": [
    "autocorrect(\"I seeı it\", vocab, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "72e2d767-3ff8-4ccb-bffa-22364b0b2f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  life is a diink annd lve is a drrug\n",
      "Wrong words :  ['diink', 'annd', 'lve', 'drrug']\n",
      "Output Sentence :  Life is a think and love is a drug\n"
     ]
    }
   ],
   "source": [
    "autocorrect(\"life is a diink annd lve is a drrug\", vocab, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04912b8a-2813-4888-8990-b441dc1d070a",
   "metadata": {},
   "source": [
    "### Improvement 1 : Introducing n-gram probabilities to get context from previous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "77a4bc03-4f80-4e8a-85a9-67ba791a693c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \n",
    "    # Initialize dictionary of n-grams and their counts\n",
    "    n_grams = {}\n",
    "\n",
    "    \n",
    "    for sentence in data: \n",
    "        \n",
    "        # prepend start token n times, and  append <e> one time\n",
    "        sentence = [start_token]*n + sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        for i in range(len(sentence)-n): \n",
    "            n_gram = sentence[i:i+n]\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f2ecf811-3832-4443-a0a8-2a8159aa67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOME UTILITY\n",
    "\n",
    "def split_to_sentences(data):\n",
    "    #sentences = data.split(\"\\n\")\n",
    "    sentences = [s.strip() for s in data]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences    \n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = nltk.tokenize.word_tokenize(sentence)\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def get_tokenized_data(data):\n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b11a6c70-b2f5-434e-964f-0c3dcdec8c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(file)\n",
    "bigram_counts = count_n_grams(tokenized_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e91799e3-c9b4-4d12-8abb-f44864cb13e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bigram_prob(word, prev_word, bigram_counts, factor):\n",
    "    key = tuple([prev_word, word])\n",
    "    #print(key)  \n",
    "    \n",
    "    ksum = 0\n",
    "    occ = 0\n",
    "    for k, v in bigram_counts.items():\n",
    "        if k[0] == prev_word:\n",
    "            ksum = ksum + v\n",
    "            occ = occ + 1\n",
    "    #print(ksum)\n",
    "    #print(occ)\n",
    "    \n",
    "    count = 0\n",
    "    if key in bigram_counts.keys():\n",
    "        count = bigram_counts[key]\n",
    "    #print(type(occ))\n",
    "    \n",
    "    smooth_count = count + factor \n",
    "    smooth_occ = ksum + occ*factor\n",
    "    probability = smooth_count / smooth_occ\n",
    "    #print(probability)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d795e439-1fd3-4706-b1fa-02025a3efae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026978950489178772"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bigram_prob('is', 'that', bigram_counts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bb9c7d31-5a88-40bc-9732-9b1dbb38b729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_corrections_bigram(word, prev_word, probs, vocab, bigram_counts, unigram_weight=0.3, bigram_weight=0.7, n=5, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "   \n",
    "    if word in probs.keys():\n",
    "        suggestions.append(word)\n",
    "    for w in edit_one_letter(word):\n",
    "        if len(suggestions) == n:\n",
    "            break\n",
    "        if w in probs.keys():\n",
    "            suggestions.append(w)\n",
    "    for w in edit_two_letters(word):\n",
    "        if len(suggestions) == n:\n",
    "            break\n",
    "        if w in probs.keys():\n",
    "            suggestions.append(w)\n",
    "        \n",
    "        \n",
    "    best_words = {}\n",
    "    \n",
    "    for s in suggestions:\n",
    "        #best_words[s] = probs[s]\n",
    "        unigram_prob = probs[s]\n",
    "        #print(s)\n",
    "        try:\n",
    "            bigram_prob = get_bigram_prob(s, prev_word, bigram_counts, 1)\n",
    "        except:\n",
    "            bigram_prob = 0.0000000000000000001\n",
    "\n",
    "        final_score = unigram_weight*unigram_prob + bigram_weight*bigram_prob\n",
    "        \n",
    "        best_words[s] = final_score     \n",
    "        \n",
    "    best_words = sorted(best_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    n_best = best_words \n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9b0e4080-a3bc-4de3-ab3a-0d966d8ffdab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_correct_word_bigram(word, prev_word, probs, vocab, bigram_counts, unigram_weight, bigram_weight, n): \n",
    "    corrections = get_corrections_bigram(word, prev_word, probs, vocab, \n",
    "                                         bigram_counts, unigram_weight, bigram_weight, n, verbose=False)\n",
    "    #print(corrections)\n",
    "    if len(corrections) == 0:\n",
    "        return word\n",
    "    \n",
    "    final_word = corrections[0][0]\n",
    "    final_prob = corrections[0][1]\n",
    "    for i, word_prob in enumerate(corrections):\n",
    "        #print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "        if word_prob[1] > final_prob:\n",
    "            final_word = word_prob[0]\n",
    "            final_prob = word_prob[1]\n",
    "    return final_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1986c290-e75d-4137-8763-cd40b7ba8aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autocorrect_bigram(sentence, vocab, probs, bigram_counts):\n",
    "    print(\"Input sentence : \", sentence)\n",
    "    wrong_words = find_wrong_word(sentence, vocab)\n",
    "    print(\"Wrong words : \", wrong_words)\n",
    "    #print(wrong_words)\n",
    "    correct_words = []\n",
    "    word_list = sentence.strip().lower().split(\" \")\n",
    "    for i, word in enumerate(word_list):\n",
    "        #print(i, word)\n",
    "        \n",
    "        #### Previous word\n",
    "        if i==0:\n",
    "            prev_word = '<s>'\n",
    "        else:\n",
    "            prev_word = word_list[i-1]\n",
    "            \n",
    "        if word in wrong_words:\n",
    "            correct_word = get_correct_word_bigram(word, prev_word, probs, vocab, bigram_counts, 0.3, 0.7, 10)\n",
    "            #print(word, correct_word)\n",
    "            word = correct_word\n",
    "        correct_words.append(word)\n",
    "    print(\"Output Sentence : \", \" \".join(correct_words).capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0047db00-cd54-433d-b92e-df6bb7c0a853",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  she is really beutifule\n",
      "Wrong words :  ['beutifule']\n",
      "Output Sentence :  She is really beatiful\n"
     ]
    }
   ],
   "source": [
    "autocorrect_bigram('she is really beutifule', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d9434d3e-ece0-472b-82cb-b717e8da3e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  you are not alowwed here\n",
      "Wrong words :  ['alowwed']\n",
      "Output Sentence :  You are not allowed here\n"
     ]
    }
   ],
   "source": [
    "autocorrect_bigram('you are not alowwed here', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f5b854a2-db92-4bc8-905d-3347689b08b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  physics is the most amainzg subect\n",
      "Wrong words :  ['amainzg', 'subect']\n",
      "Output Sentence :  Physics is the most amazing subject\n"
     ]
    }
   ],
   "source": [
    "autocorrect_bigram('physics is the most amainzg subect', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4092f2-a046-4ac6-b801-d1e6f57af64f",
   "metadata": {},
   "source": [
    "### Improvement 2 : Introducing min_edit_diatsnce functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "045983bd-a6f1-4dd4-914d-4c323a09f6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    \n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    for row in range(1,m+1): \n",
    "        D[row,0] = D[row-1, 0] + del_cost\n",
    "        \n",
    "    for col in range(1,n+1):\n",
    "        D[0,col] = D[0, col-1] + ins_cost\n",
    "        \n",
    "    # Loop through row 1 to row m\n",
    "    for row in range(1,m+1): \n",
    "        # Loop through column 1 to column n\n",
    "        for col in range(1,n+1):\n",
    "            # Intialize r_cost to the 'replace' cost \n",
    "            r_cost = rep_cost\n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            D[row,col] = D[row-1][col-1] + r_cost\n",
    "          \n",
    "    # Set the minimum edit distance with the cost found at row m, column n\n",
    "    med = D[m][n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "604d8bc7-60b7-4ef3-ac44-76bc53f1fd5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_correct_word_bigram_min_edit(word, prev_word, probs, vocab, bigram_counts, unigram_weight, bigram_weight, n, scale_dist): \n",
    "    corrections = get_corrections_bigram(word, prev_word, probs, vocab, \n",
    "                                         bigram_counts, unigram_weight, bigram_weight, n, verbose=False)\n",
    "    #print(corrections)\n",
    "    if len(corrections) == 0:\n",
    "        return word\n",
    "    \n",
    "    ### Make a dataframe of suggestions\n",
    "    words = []\n",
    "    probabs = []\n",
    "    dist = []\n",
    "    for pair in corrections:\n",
    "        words.append(pair[0])\n",
    "        probabs.append(pair[1])\n",
    "        _, distance = min_edit_distance(word, pair[0], 1, 1, 2)\n",
    "        dist.append(distance)\n",
    "        \n",
    "    df = pd.DataFrame({'suggestion':words, 'distance':dist, 'probability':probabs})\n",
    "    df['inv_dist'] = df['distance'].apply(lambda x : (1/x)*scale_dist)\n",
    "    df['score'] = df['inv_dist'] + df['probability']\n",
    "    df = df.sort_values(by='score', ascending=False)\n",
    "    #df = df.sort_values(by=['distance', 'probability'], ascending=[True, False])\n",
    "    #display(df)\n",
    "    \n",
    "    final_word = df.iloc[0,0]\n",
    "    \n",
    "    return final_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1bb93ba0-dffb-4bdc-897e-c20853b94231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'policy'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_correct_word_bigram_min_edit('pulicy', 'best', probs, vocab, bigram_counts, 0.3, 0.7, 10, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d7e77846-7e14-4181-bcb5-3ab4837f07e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autocorrect_bigram_min_edit(sentence, vocab, probs, bigram_probability_df, scale_dist=0.001):\n",
    "    print(\"Input sentence : \", sentence)\n",
    "    wrong_words = find_wrong_word(sentence, vocab)\n",
    "    print(\"Wrong words : \", wrong_words)\n",
    "    #print(wrong_words)\n",
    "    correct_words = []\n",
    "    word_list = sentence.strip().lower().split(\" \")\n",
    "    for i, word in enumerate(word_list):\n",
    "        #print(i, word)\n",
    "        \n",
    "        #### Previous word\n",
    "        if i==0:\n",
    "            prev_word = '<s>'\n",
    "        else:\n",
    "            prev_word = word_list[i-1]\n",
    "            \n",
    "        if word in wrong_words:\n",
    "            correct_word = get_correct_word_bigram_min_edit(word, prev_word, probs, vocab, bigram_probability_df, 0.3, 0.7, 25, scale_dist)\n",
    "            #print(word, correct_word)\n",
    "            word = correct_word\n",
    "        correct_words.append(word)\n",
    "    print(\"Output Sentence : \", \" \".join(correct_words).capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "77acdf73-7957-4036-9b0e-850c7a1e5118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  I have acess to the lidrary\n",
      "Wrong words :  ['lidrary']\n",
      "Output Sentence :  I have acess to the library\n"
     ]
    }
   ],
   "source": [
    "autocorrect_bigram_min_edit('I have acess to the lidrary', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ff6e4989-7f6f-4165-ac09-215a3cfacd15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  presnet for the meeating\n",
      "Wrong words :  ['presnet', 'meeating']\n",
      "Output Sentence :  Present for the meaning\n"
     ]
    }
   ],
   "source": [
    "autocorrect_bigram_min_edit('presnet for the meeating', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "90f633dc-54c7-456a-ac83-94e3c951de29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  he planed a game\n",
      "Wrong words :  []\n",
      "Output Sentence :  He planed a game\n"
     ]
    }
   ],
   "source": [
    "autocorrect_bigram_min_edit('he planed a game', vocab, probs, bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1813e9-729d-474e-b4d7-9295b2c09116",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Let's design a unit test for the auto-correct system that we have develpoed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2efd04e1-ff4d-4b8c-846e-2dffb7173480",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tests1 = {'access': 'acess',\n",
    "          'accessing': 'accesing',\n",
    "          'accommodation': 'accomodation acommodation acomodation',\n",
    "          'account': 'acount',\n",
    "          'address': 'adress adres',\n",
    "          'addressable': 'addresable',\n",
    "          'arranged': 'aranged arrainged',\n",
    "          'arrangeing': 'aranging',\n",
    "          'arrangement': 'arragment',\n",
    "          'articles': 'articals',\n",
    "          'aunt': 'annt anut arnt',\n",
    "          'auxiliary': 'auxillary',\n",
    "          'available': 'avaible',\n",
    "          'awful': 'awfall afful',\n",
    "          'basically': 'basicaly',\n",
    "          'beginning': 'begining',\n",
    "          'benefit': 'benifit',\n",
    "          'benefits': 'benifits',\n",
    "          'between': 'beetween',\n",
    "          'bicycle': 'bicycal bycicle bycycle',\n",
    "          'biscuits': 'biscits biscutes biscuts bisquits buiscits buiscuts',\n",
    "          'built': 'biult',\n",
    "          'cake': 'cak',\n",
    "          'career': 'carrer',\n",
    "          'cemetery': 'cemetary semetary',\n",
    "          'centrally': 'centraly',\n",
    "          'certain': 'cirtain',\n",
    "          'challenges': 'chalenges chalenges',\n",
    "          'chapter': 'chaper chaphter chaptur',\n",
    "          'choice': 'choise',\n",
    "          'choosing': 'chosing',\n",
    "          'clerical': 'clearical',\n",
    "          'committee': 'comittee',\n",
    "          'compare': 'compair',\n",
    "          'completely': 'completly',\n",
    "          'consider': 'concider',\n",
    "          'considerable': 'conciderable',\n",
    "          'contented': 'contenpted contende contended contentid',\n",
    "          'curtains': 'cartains certans courtens cuaritains curtans curtians curtions',\n",
    "          'decide': 'descide',\n",
    "          'decided': 'descided',\n",
    "          'definitely': 'definately difinately',\n",
    "          'definition': 'defenition',\n",
    "          'definitions': 'defenitions',\n",
    "          'description': 'discription',\n",
    "          'desiccate': 'desicate dessicate dessiccate',\n",
    "          'diagrammatically': 'diagrammaticaally',\n",
    "          'different': 'diffrent',\n",
    "          'driven': 'dirven',\n",
    "          'ecstasy': 'exstacy ecstacy',\n",
    "          'embarrass': 'embaras embarass',\n",
    "          'establishing': 'astablishing establising',\n",
    "          'experience': 'experance experiance',\n",
    "          'experiences': 'experances',\n",
    "          'extended': 'extented',\n",
    "          'extremely': 'extreamly',\n",
    "          'fails': 'failes',\n",
    "          'families': 'familes',\n",
    "          'february': 'febuary',\n",
    "          'further': 'futher',\n",
    "          'gallery': 'galery gallary gallerry gallrey',\n",
    "          'hierarchal': 'hierachial',\n",
    "          'hierarchy': 'hierchy',\n",
    "          'inconvenient': 'inconvienient inconvient inconvinient',\n",
    "          'independent': 'independant independant',\n",
    "          'initial': 'intial',\n",
    "          'initials': 'inetials inistals initails initals intials',\n",
    "          'juice': 'guic juce jucie juise juse',\n",
    "          'latest': 'lates latets latiest latist',\n",
    "          'laugh': 'lagh lauf laught lugh',\n",
    "          'level': 'leval',\n",
    "          'levels': 'levals',\n",
    "          'liaison': 'liaision liason',\n",
    "          'lieu': 'liew',\n",
    "          'literature': 'litriture',\n",
    "          'loans': 'lones',\n",
    "          'locally': 'localy',\n",
    "          'magnificent': 'magnificnet magificent magnifcent magnifecent magnifiscant magnifisent magnificant',\n",
    "          'management': 'managment',\n",
    "          'meant': 'ment',\n",
    "          'minuscule': 'miniscule',\n",
    "          'minutes': 'muinets',\n",
    "          'monitoring': 'monitering',\n",
    "          'necessary': 'neccesary necesary neccesary necassary necassery neccasary',\n",
    "          'occurrence': 'occurence occurence',\n",
    "          'often': 'ofen offen offten ofton',\n",
    "          'opposite': 'opisite oppasite oppesite oppisit oppisite opposit oppossite oppossitte',\n",
    "          'parallel': 'paralel paralell parrallel parralell parrallell',\n",
    "          'particular': 'particulaur',\n",
    "          'perhaps': 'perhapse',\n",
    "          'personnel': 'personnell',\n",
    "          'planned': 'planed',\n",
    "          'poem': 'poame',\n",
    "          'poems': 'poims pomes',\n",
    "          'poetry': 'poartry poertry poetre poety powetry',\n",
    "          'position': 'possition',\n",
    "          'possible': 'possable',\n",
    "          'pretend': 'pertend protend prtend pritend',\n",
    "          'problem': 'problam proble promblem proplen',\n",
    "          'pronunciation': 'pronounciation',\n",
    "          'purple': 'perple perpul poarple',\n",
    "          'questionnaire': 'questionaire',\n",
    "          'really': 'realy relley relly',\n",
    "          'receipt': 'receit receite reciet recipt',\n",
    "          'receive': 'recieve',\n",
    "          'refreshment': 'reafreshment refreshmant refresment refressmunt',\n",
    "          'remember': 'rember remeber rememmer rermember',\n",
    "          'remind': 'remine remined',\n",
    "          'scarcely': 'scarcly scarecly scarely scarsely',\n",
    "          'scissors': 'scisors sissors',\n",
    "          'separate': 'seperate',\n",
    "          'singular': 'singulaur',\n",
    "          'someone': 'somone',\n",
    "          'sources': 'sorces',\n",
    "          'southern': 'southen',\n",
    "          'special': 'speaical specail specal speical',\n",
    "          'splendid': 'spledid splended splened splended',\n",
    "          'standardizing': 'stanerdizing',\n",
    "          'stomach': 'stomac stomache stomec stumache',\n",
    "          'supersede': 'supercede superceed',\n",
    "          'there': 'ther',\n",
    "          'totally': 'totaly',\n",
    "          'transferred': 'transfred',\n",
    "          'transportability': 'transportibility',\n",
    "          'triangular': 'triangulaur',\n",
    "          'understand': 'undersand undistand',\n",
    "          'unexpected': 'unexpcted unexpeted unexspected',\n",
    "          'unfortunately': 'unfortunatly',\n",
    "          'unique': 'uneque',\n",
    "          'useful': 'usefull',\n",
    "          'valuable': 'valubale valuble',\n",
    "          'variable': 'varable',\n",
    "          'variant': 'vairiant',\n",
    "          'various': 'vairious',\n",
    "          'visited': 'fisited viseted vistid vistied',\n",
    "          'visitors': 'vistors',\n",
    "          'voluntary': 'volantry',\n",
    "          'voting': 'voteing',\n",
    "          'wanted': 'wantid wonted',\n",
    "          'whether': 'wether',\n",
    "          'wrote': 'rote wote'}\n",
    "\n",
    "tests2 = {'forbidden': 'forbiden',\n",
    "          'decisions': 'deciscions descisions',\n",
    "          'supposedly': 'supposidly',\n",
    "          'embellishing': 'embelishing',\n",
    "          'technique': 'tecnique',\n",
    "          'permanently': 'perminantly',\n",
    "          'confirmation': 'confermation',\n",
    "          'appointment': 'appoitment',\n",
    "          'progression': 'progresion',\n",
    "          'accompanying': 'acompaning',\n",
    "          'applicable': 'aplicable',\n",
    "          'regained': 'regined',\n",
    "          'guidelines': 'guidlines',\n",
    "          'surrounding': 'serounding',\n",
    "          'titles': 'tittles',\n",
    "          'unavailable': 'unavailble',\n",
    "          'advantageous': 'advantageos',\n",
    "          'brief': 'brif',\n",
    "          'appeal': 'apeal',\n",
    "          'consisting': 'consisiting',\n",
    "          'clerk': 'cleark clerck',\n",
    "          'component': 'componant',\n",
    "          'favourable': 'faverable',\n",
    "          'separation': 'seperation',\n",
    "          'search': 'serch',\n",
    "          'receive': 'recieve',\n",
    "          'employees': 'emploies',\n",
    "          'prior': 'piror',\n",
    "          'resulting': 'reulting',\n",
    "          'suggestion': 'sugestion',\n",
    "          'opinion': 'oppinion',\n",
    "          'cancellation': 'cancelation',\n",
    "          'criticism': 'citisum',\n",
    "          'useful': 'usful',\n",
    "          'humour': 'humor',\n",
    "          'anomalies': 'anomolies',\n",
    "          'would': 'whould',\n",
    "          'doubt': 'doupt',\n",
    "          'examination': 'eximination',\n",
    "          'therefore': 'therefoe',\n",
    "          'recommend': 'recomend',\n",
    "          'separated': 'seperated',\n",
    "          'successful': 'sucssuful succesful',\n",
    "          'apparent': 'apparant',\n",
    "          'occurred': 'occureed',\n",
    "          'particular': 'paerticulaur',\n",
    "          'pivoting': 'pivting',\n",
    "          'announcing': 'anouncing',\n",
    "          'challenge': 'chalange',\n",
    "          'arrangements': 'araingements',\n",
    "          'proportions': 'proprtions',\n",
    "          'organized': 'oranised',\n",
    "          'accept': 'acept',\n",
    "          'dependence': 'dependance',\n",
    "          'unequalled': 'unequaled',\n",
    "          'numbers': 'numbuers',\n",
    "          'sense': 'sence',\n",
    "          'conversely': 'conversly',\n",
    "          'provide': 'provid',\n",
    "          'arrangement': 'arrangment',\n",
    "          'responsibilities': 'responsiblities',\n",
    "          'fourth': 'forth',\n",
    "          'ordinary': 'ordenary',\n",
    "          'description': 'desription descvription desacription',\n",
    "          'inconceivable': 'inconcievable',\n",
    "          'data': 'dsata',\n",
    "          'register': 'rgister',\n",
    "          'supervision': 'supervison',\n",
    "          'encompassing': 'encompasing',\n",
    "          'negligible': 'negligable',\n",
    "          'allow': 'alow',\n",
    "          'operations': 'operatins',\n",
    "          'executed': 'executted',\n",
    "          'interpretation': 'interpritation',\n",
    "          'hierarchy': 'heiarky',\n",
    "          'indeed': 'indead',\n",
    "          'years': 'yesars',\n",
    "          'through': 'throut',\n",
    "          'committee': 'committe',\n",
    "          'inquiries': 'equiries',\n",
    "          'before': 'befor',\n",
    "          'continued': 'contuned',\n",
    "          'permanent': 'perminant',\n",
    "          'choose': 'chose',\n",
    "          'virtually': 'vertually',\n",
    "          'correspondence': 'correspondance',\n",
    "          'eventually': 'eventully',\n",
    "          'lonely': 'lonley',\n",
    "          'profession': 'preffeson',\n",
    "          'they': 'thay',\n",
    "          'now': 'noe',\n",
    "          'desperately': 'despratly',\n",
    "          'university': 'unversity',\n",
    "          'adjournment': 'adjurnment',\n",
    "          'possibilities': 'possablities',\n",
    "          'stopped': 'stoped',\n",
    "          'mean': 'meen',\n",
    "          'weighted': 'wagted',\n",
    "          'adequately': 'adequattly',\n",
    "          'shown': 'hown',\n",
    "          'matrix': 'matriiix',\n",
    "          'profit': 'proffit',\n",
    "          'encourage': 'encorage',\n",
    "          'collate': 'colate',\n",
    "          'disaggregate': 'disaggreagte disaggreaget',\n",
    "          'receiving': 'recieving reciving',\n",
    "          'proviso': 'provisoe',\n",
    "          'umbrella': 'umberalla',\n",
    "          'approached': 'aproached',\n",
    "          'pleasant': 'plesent',\n",
    "          'difficulty': 'dificulty',\n",
    "          'appointments': 'apointments',\n",
    "          'base': 'basse',\n",
    "          'conditioning': 'conditining',\n",
    "          'earliest': 'earlyest',\n",
    "          'beginning': 'begining',\n",
    "          'universally': 'universaly',\n",
    "          'unresolved': 'unresloved',\n",
    "          'length': 'lengh',\n",
    "          'exponentially': 'exponentualy',\n",
    "          'utilized': 'utalised',\n",
    "          'set': 'et',\n",
    "          'surveys': 'servays',\n",
    "          'families': 'familys',\n",
    "          'system': 'sysem',\n",
    "          'approximately': 'aproximatly',\n",
    "          'their': 'ther',\n",
    "          'scheme': 'scheem',\n",
    "          'speaking': 'speeking',\n",
    "          'repetitive': 'repetative',\n",
    "          'inefficient': 'ineffiect',\n",
    "          'geneva': 'geniva',\n",
    "          'exactly': 'exsactly',\n",
    "          'immediate': 'imediate',\n",
    "          'appreciation': 'apreciation',\n",
    "          'luckily': 'luckeley',\n",
    "          'eliminated': 'elimiated',\n",
    "          'believe': 'belive',\n",
    "          'appreciated': 'apreciated',\n",
    "          'readjusted': 'reajusted',\n",
    "          'were': 'wer where',\n",
    "          'feeling': 'fealing',\n",
    "          'and': 'anf',\n",
    "          'false': 'faulse',\n",
    "          'seen': 'seeen',\n",
    "          'interrogating': 'interogationg',\n",
    "          'academically': 'academicly',\n",
    "          'relatively': 'relativly relitivly',\n",
    "          'traditionally': 'traditionaly',\n",
    "          'studying': 'studing',\n",
    "          'majority': 'majorty',\n",
    "          'build': 'biuld',\n",
    "          'aggravating': 'agravating',\n",
    "          'transactions': 'trasactions',\n",
    "          'arguing': 'aurguing',\n",
    "          'sheets': 'sheertes',\n",
    "          'successive': 'sucsesive sucessive',\n",
    "          'segment': 'segemnt',\n",
    "          'especially': 'especaily',\n",
    "          'later': 'latter',\n",
    "          'senior': 'sienior',\n",
    "          'dragged': 'draged',\n",
    "          'atmosphere': 'atmospher',\n",
    "          'drastically': 'drasticaly',\n",
    "          'particularly': 'particulary',\n",
    "          'visitor': 'vistor',\n",
    "          'session': 'sesion',\n",
    "          'continually': 'contually',\n",
    "          'availability': 'avaiblity',\n",
    "          'busy': 'buisy',\n",
    "          'parameters': 'perametres',\n",
    "          'surroundings': 'suroundings seroundings',\n",
    "          'employed': 'emploied',\n",
    "          'adequate': 'adiquate',\n",
    "          'handle': 'handel',\n",
    "          'means': 'meens',\n",
    "          'familiar': 'familer',\n",
    "          'between': 'beeteen',\n",
    "          'overall': 'overal',\n",
    "          'timing': 'timeing',\n",
    "          'committees': 'comittees commitees',\n",
    "          'queries': 'quies',\n",
    "          'econometric': 'economtric',\n",
    "          'erroneous': 'errounous',\n",
    "          'decides': 'descides',\n",
    "          'reference': 'refereence refference',\n",
    "          'intelligence': 'inteligence',\n",
    "          'edition': 'ediion ediition',\n",
    "          'are': 'arte',\n",
    "          'apologies': 'appologies',\n",
    "          'thermawear': 'thermawere thermawhere',\n",
    "          'techniques': 'tecniques',\n",
    "          'voluntary': 'volantary',\n",
    "          'subsequent': 'subsequant subsiquent',\n",
    "          'currently': 'curruntly',\n",
    "          'forecast': 'forcast',\n",
    "          'weapons': 'wepons',\n",
    "          'routine': 'rouint',\n",
    "          'neither': 'niether',\n",
    "          'approach': 'aproach',\n",
    "          'available': 'availble',\n",
    "          'recently': 'reciently',\n",
    "          'ability': 'ablity',\n",
    "          'nature': 'natior',\n",
    "          'commercial': 'comersial',\n",
    "          'agencies': 'agences',\n",
    "          'however': 'howeverr',\n",
    "          'suggested': 'sugested',\n",
    "          'career': 'carear',\n",
    "          'many': 'mony',\n",
    "          'annual': 'anual',\n",
    "          'according': 'acording',\n",
    "          'receives': 'recives recieves',\n",
    "          'interesting': 'intresting',\n",
    "          'expense': 'expence',\n",
    "          'relevant': 'relavent relevaant',\n",
    "          'table': 'tasble',\n",
    "          'throughout': 'throuout',\n",
    "          'conference': 'conferance',\n",
    "          'sensible': 'sensable',\n",
    "          'described': 'discribed describd',\n",
    "          'union': 'unioun',\n",
    "          'interest': 'intrest',\n",
    "          'flexible': 'flexable',\n",
    "          'refered': 'reffered',\n",
    "          'controlled': 'controled',\n",
    "          'sufficient': 'suficient',\n",
    "          'dissension': 'desention',\n",
    "          'adaptable': 'adabtable',\n",
    "          'representative': 'representitive',\n",
    "          'irrelevant': 'irrelavent',\n",
    "          'unnecessarily': 'unessasarily',\n",
    "          'applied': 'upplied',\n",
    "          'apologised': 'appologised',\n",
    "          'these': 'thees thess',\n",
    "          'choices': 'choises',\n",
    "          'will': 'wil',\n",
    "          'procedure': 'proceduer',\n",
    "          'shortened': 'shortend',\n",
    "          'manually': 'manualy',\n",
    "          'disappointing': 'dissapoiting',\n",
    "          'excessively': 'exessively',\n",
    "          'comments': 'coments',\n",
    "          'containing': 'containg',\n",
    "          'develop': 'develope',\n",
    "          'credit': 'creadit',\n",
    "          'government': 'goverment',\n",
    "          'acquaintances': 'aquantences',\n",
    "          'orientated': 'orentated',\n",
    "          'widely': 'widly',\n",
    "          'advise': 'advice',\n",
    "          'difficult': 'dificult',\n",
    "          'investigated': 'investegated',\n",
    "          'bonus': 'bonas',\n",
    "          'conceived': 'concieved',\n",
    "          'nationally': 'nationaly',\n",
    "          'compared': 'comppared compased',\n",
    "          'moving': 'moveing',\n",
    "          'necessity': 'nessesity',\n",
    "          'opportunity': 'oppertunity oppotunity opperttunity',\n",
    "          'thoughts': 'thorts',\n",
    "          'equalled': 'equaled',\n",
    "          'variety': 'variatry',\n",
    "          'analysis': 'analiss analsis analisis',\n",
    "          'patterns': 'pattarns',\n",
    "          'qualities': 'quaties',\n",
    "          'easily': 'easyly',\n",
    "          'organization': 'oranisation oragnisation',\n",
    "          'the': 'thw hte thi',\n",
    "          'corporate': 'corparate',\n",
    "          'composed': 'compossed',\n",
    "          'enormously': 'enomosly',\n",
    "          'financially': 'financialy',\n",
    "          'functionally': 'functionaly',\n",
    "          'discipline': 'disiplin',\n",
    "          'announcement': 'anouncement',\n",
    "          'progresses': 'progressess',\n",
    "          'except': 'excxept',\n",
    "          'recommending': 'recomending',\n",
    "          'mathematically': 'mathematicaly',\n",
    "          'source': 'sorce',\n",
    "          'combine': 'comibine',\n",
    "          'input': 'inut',\n",
    "          'careers': 'currers carrers',\n",
    "          'resolved': 'resoved',\n",
    "          'demands': 'diemands',\n",
    "          'unequivocally': 'unequivocaly',\n",
    "          'suffering': 'suufering',\n",
    "          'immediately': 'imidatly imediatly',\n",
    "          'accepted': 'acepted',\n",
    "          'projects': 'projeccts',\n",
    "          'necessary': 'necasery nessasary nessisary neccassary',\n",
    "          'journalism': 'journaism',\n",
    "          'unnecessary': 'unessessay',\n",
    "          'night': 'nite',\n",
    "          'output': 'oputput',\n",
    "          'security': 'seurity',\n",
    "          'essential': 'esential',\n",
    "          'beneficial': 'benificial benficial',\n",
    "          'explaining': 'explaning',\n",
    "          'supplementary': 'suplementary',\n",
    "          'questionnaire': 'questionare',\n",
    "          'employment': 'empolyment',\n",
    "          'proceeding': 'proceding',\n",
    "          'decision': 'descisions descision',\n",
    "          'per': 'pere',\n",
    "          'discretion': 'discresion',\n",
    "          'reaching': 'reching',\n",
    "          'analysed': 'analised',\n",
    "          'expansion': 'expanion',\n",
    "          'although': 'athough',\n",
    "          'subtract': 'subtrcat',\n",
    "          'analysing': 'aalysing',\n",
    "          'comparison': 'comparrison',\n",
    "          'months': 'monthes',\n",
    "          'hierarchal': 'hierachial',\n",
    "          'misleading': 'missleading',\n",
    "          'commit': 'comit',\n",
    "          'auguments': 'aurgument',\n",
    "          'within': 'withing',\n",
    "          'obtaining': 'optaning',\n",
    "          'accounts': 'acounts',\n",
    "          'primarily': 'pimarily',\n",
    "          'operator': 'opertor',\n",
    "          'accumulated': 'acumulated',\n",
    "          'extremely': 'extreemly',\n",
    "          'there': 'thear',\n",
    "          'summarys': 'sumarys',\n",
    "          'analyse': 'analiss',\n",
    "          'understandable': 'understadable',\n",
    "          'safeguard': 'safegaurd',\n",
    "          'consist': 'consisit',\n",
    "          'declarations': 'declaratrions',\n",
    "          'minutes': 'muinutes muiuets',\n",
    "          'associated': 'assosiated',\n",
    "          'accessibility': 'accessability',\n",
    "          'examine': 'examin',\n",
    "          'surveying': 'servaying',\n",
    "          'politics': 'polatics',\n",
    "          'annoying': 'anoying',\n",
    "          'again': 'agiin',\n",
    "          'assessing': 'accesing',\n",
    "          'ideally': 'idealy',\n",
    "          'scrutinized': 'scrutiniesed',\n",
    "          'simular': 'similar',\n",
    "          'personnel': 'personel',\n",
    "          'whereas': 'wheras',\n",
    "          'when': 'whn',\n",
    "          'geographically': 'goegraphicaly',\n",
    "          'gaining': 'ganing',\n",
    "          'requested': 'rquested',\n",
    "          'separate': 'seporate',\n",
    "          'students': 'studens',\n",
    "          'prepared': 'prepaired',\n",
    "          'generated': 'generataed',\n",
    "          'graphically': 'graphicaly',\n",
    "          'suited': 'suted',\n",
    "          'variable': 'varible vaiable',\n",
    "          'building': 'biulding',\n",
    "          'required': 'reequired',\n",
    "          'necessitates': 'nessisitates',\n",
    "          'together': 'togehter',\n",
    "          'profits': 'proffits'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090f669-6629-4456-9f8e-59b8c4c8ad64",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "de976cfb-9fc2-4838-b84c-9070fcdfb71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_autocorrect(utdata, vocab, probs, string):\n",
    "    tcount = 0\n",
    "    fcount = 0\n",
    "    rcount = 0\n",
    "    print(\"Running \"+string+\" : Basic Auto-correct system\")\n",
    "    for k, v in utdata.items():\n",
    "        incorrect_list = v.strip().split()\n",
    "        #print(incorrect_list)\n",
    "        for w in incorrect_list:\n",
    "            tcount = tcount + 1\n",
    "            cw = get_correct_word(w, vocab, probs, 25)\n",
    "            if cw==k:\n",
    "                #print('correct')\n",
    "                rcount = rcount + 1\n",
    "            else:\n",
    "                #print('wrong')\n",
    "                fcount = fcount + 1\n",
    "    print(\"Accuracy : {} %\".format((rcount/tcount)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "55cff3ac-567a-4e57-b3f3-97e4127cd447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_autocorrect_bigram(utdata, vocab, probs, string, bigram_counts):\n",
    "    tcount = 0\n",
    "    fcount = 0\n",
    "    rcount = 0\n",
    "    print(\"Running \"+string+\" : Bi-gram Auto-correct system\")\n",
    "    for k, v in utdata.items():\n",
    "        incorrect_list = v.strip().split()\n",
    "        #print(incorrect_list)\n",
    "        for w in incorrect_list:\n",
    "            tcount = tcount + 1\n",
    "            cw = get_correct_word_bigram(w, '<s>', probs, vocab, bigram_counts, 0.3, 0.7, 25)\n",
    "            if cw==k:\n",
    "                #print('correct')\n",
    "                rcount = rcount + 1\n",
    "            else:\n",
    "                #print('wrong')\n",
    "                fcount = fcount + 1\n",
    "    print(\"Accuracy : {} %\".format((rcount/tcount)*100))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "75f90591-9054-4a8d-a673-75a7b6828af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_autocorrect_bigram_min_edit(utdata, vocab, probs, string, bigram_counts):\n",
    "    tcount = 0\n",
    "    fcount = 0\n",
    "    rcount = 0\n",
    "    print(\"Running \"+string+\" : Bi-gram (min-edit) Auto-correct system\")\n",
    "    for k, v in utdata.items():\n",
    "        incorrect_list = v.strip().split()\n",
    "        #print(incorrect_list)\n",
    "        for w in incorrect_list:\n",
    "            tcount = tcount + 1\n",
    "            cw = get_correct_word_bigram_min_edit(w, '<s>', probs, vocab, bigram_counts, 0.3, 0.7, 25, 0.000001)\n",
    "            if cw==k:\n",
    "                #print('correct')\n",
    "                rcount = rcount + 1\n",
    "            else:\n",
    "                #print('wrong')\n",
    "                fcount = fcount + 1\n",
    "    print(\"Accuracy : {} %\".format((rcount/tcount)*100))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a7da5819-8e1e-4615-9d28-55442c7a6be8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Unit Test 1 : Basic Auto-correct system\n",
      "Accuracy : 55.18518518518518 %\n",
      "Running Unit Test 2 : Basic Auto-correct system\n",
      "Accuracy : 45.5 %\n"
     ]
    }
   ],
   "source": [
    "# Test basic auto-correct\n",
    "test_autocorrect(tests1, vocab, probs, \"Unit Test 1\")\n",
    "test_autocorrect(tests2, vocab, probs, \"Unit Test 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8600673c-a7c1-4db8-babe-82c73dd1a790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Unit Test 1 : Bi-gram Auto-correct system\n",
      "Accuracy : 61.85185185185185 %\n",
      "Running Unit Test 2 : Bi-gram Auto-correct system\n",
      "Accuracy : 54.75 %\n"
     ]
    }
   ],
   "source": [
    "test_autocorrect_bigram(tests1, vocab, probs, \"Unit Test 1\", bigram_counts)\n",
    "test_autocorrect_bigram(tests2, vocab, probs, \"Unit Test 2\", bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b1119886-3677-4ca6-aef1-8ea1a528350a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_autocorrect_bigram_min_edit(tests1, vocab, probs, \"Unit Test 1\", bigram_counts)\n",
    "#test_autocorrect_bigram_min_edit(tests2, vocab, probs, \"Unit Test 2\", bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4c1f6-c7f9-4596-b096-b29ed98df2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
